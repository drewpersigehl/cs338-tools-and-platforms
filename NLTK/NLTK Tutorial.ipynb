{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63e21889",
   "metadata": {},
   "source": [
    "## Install and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9ddceb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\danie\\miniconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\danie\\miniconda3\\lib\\site-packages (from nltk) (2021.11.2)\n",
      "Requirement already satisfied: click in c:\\users\\danie\\miniconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\danie\\miniconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\danie\\miniconda3\\lib\\site-packages (from nltk) (4.51.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\danie\\miniconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# install nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "685fa2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178fc50",
   "metadata": {},
   "source": [
    "## Sentiment Analysis through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60bfc4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.283, 'pos': 0.717, 'compound': 0.7249}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# a positive example\n",
    "text1 = \"I love thie movie!!!\"\n",
    "\n",
    "# a negative example\n",
    "text2 = \"I hate this movie!!!\"\n",
    "\n",
    "# a neutual example\n",
    "text3 = \"I watched a movie.\"\n",
    "\n",
    "# use polarity_scores function to return the dictionary of values\n",
    "dict1 = sia.polarity_scores(text1)\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00a5763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.696, 'neu': 0.304, 'pos': 0.0, 'compound': -0.6784}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict2 = sia.polarity_scores(text2)\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3f7f533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict3 = sia.polarity_scores(text3)\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f4fc68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment score for 'I love thie movie!!!' is 0.7249.\n",
      "Sentiment score for 'I hate this movie!!!' is -0.6784.\n",
      "Sentiment score for 'I watched a movie.' is 0.0.\n"
     ]
    }
   ],
   "source": [
    "# usually, we use the compound score as the overall sentiment score for a text\n",
    "print(f\"Sentiment score for '{text1}' is {dict1['compound']}.\")\n",
    "print(f\"Sentiment score for '{text2}' is {dict2['compound']}.\")\n",
    "print(f\"Sentiment score for '{text3}' is {dict3['compound']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "634e3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I love thie movie!!!' is classified as pos.\n",
      "'I hate this movie!!!' is classified as neg.\n",
      "'I watched a movie.' is classified as neu.\n"
     ]
    }
   ],
   "source": [
    "# ulternatively, we could use the category that has the largest value to classify a text\n",
    "print(f\"'{text1}' is classified as {list(dict1.keys())[list(dict1.values()).index(max(list(dict1.values())[:-1]))]}.\")\n",
    "print(f\"'{text2}' is classified as {list(dict2.keys())[list(dict2.values()).index(max(list(dict2.values())[:-1]))]}.\")\n",
    "print(f\"'{text3}' is classified as {list(dict3.keys())[list(dict3.values()).index(max(list(dict3.values())[:-1]))]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad3ed43",
   "metadata": {},
   "source": [
    "## Datasets in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7a3e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install corpora datasets\n",
    "\n",
    "# the code below will wake the nltk data downloader that is displayed in a seperate window;\n",
    "# click the \"download\" button and wait for the download to complete;\n",
    "# close the window to finish the execution of this function;\n",
    "# should return True\n",
    "nltk.download()\n",
    "\n",
    "# the documentation for all datasets could be found here: https://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c97741c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc', 'abc.zip', 'alpino', 'alpino.zip', 'biocreative_ppi', 'biocreative_ppi.zip', 'brown', 'brown.zip', 'brown_tei', 'brown_tei.zip', 'cess_cat', 'cess_cat.zip', 'cess_esp', 'cess_esp.zip', 'chat80', 'chat80.zip', 'city_database', 'city_database.zip', 'cmudict', 'cmudict.zip', 'comparative_sentences', 'comparative_sentences.zip', 'comtrans.zip', 'conll2000', 'conll2000.zip', 'conll2002', 'conll2002.zip', 'conll2007.zip', 'crubadan', 'crubadan.zip', 'dependency_treebank', 'dependency_treebank.zip', 'dolch', 'dolch.zip', 'europarl_raw', 'europarl_raw.zip', 'extended_omw', 'extended_omw.zip', 'floresta', 'floresta.zip', 'framenet_v15', 'framenet_v15.zip', 'framenet_v17', 'framenet_v17.zip', 'gazetteers', 'gazetteers.zip', 'genesis', 'genesis.zip', 'gutenberg', 'gutenberg.zip', 'ieer', 'ieer.zip', 'inaugural', 'inaugural.zip', 'indian', 'indian.zip', 'jeita.zip', 'kimmo', 'kimmo.zip', 'knbc.zip', 'lin_thesaurus', 'lin_thesaurus.zip', 'machado.zip', 'mac_morpho', 'mac_morpho.zip', 'masc_tagged.zip', 'movie_reviews', 'movie_reviews.zip', 'mte_teip5', 'mte_teip5.zip', 'names', 'names.zip', 'nombank.1.0.zip', 'nonbreaking_prefixes', 'nonbreaking_prefixes.zip', 'nps_chat', 'nps_chat.zip', 'omw', 'omw-1.4', 'omw-1.4.zip', 'omw.zip', 'opinion_lexicon', 'opinion_lexicon.zip', 'panlex_swadesh.zip', 'paradigms', 'paradigms.zip', 'pe08', 'pe08.zip', 'pil', 'pil.zip', 'pl196x', 'pl196x.zip', 'ppattach', 'ppattach.zip', 'problem_reports', 'problem_reports.zip', 'product_reviews_1', 'product_reviews_1.zip', 'product_reviews_2', 'product_reviews_2.zip', 'propbank.zip', 'pros_cons', 'pros_cons.zip', 'ptb', 'ptb.zip', 'qc', 'qc.zip', 'reuters.zip', 'rte', 'rte.zip', 'semcor.zip', 'senseval', 'senseval.zip', 'sentence_polarity', 'sentence_polarity.zip', 'sentiwordnet', 'sentiwordnet.zip', 'shakespeare', 'shakespeare.zip', 'sinica_treebank', 'sinica_treebank.zip', 'smultron', 'smultron.zip', 'state_union', 'state_union.zip', 'stopwords', 'stopwords.zip', 'subjectivity', 'subjectivity.zip', 'swadesh', 'swadesh.zip', 'switchboard', 'switchboard.zip', 'timit', 'timit.zip', 'toolbox', 'toolbox.zip', 'treebank', 'treebank.zip', 'twitter_samples', 'twitter_samples.zip', 'udhr', 'udhr.zip', 'udhr2', 'udhr2.zip', 'unicode_samples', 'unicode_samples.zip', 'universal_treebanks_v20.zip', 'verbnet', 'verbnet.zip', 'verbnet3', 'verbnet3.zip', 'webtext', 'webtext.zip', 'wordnet', 'wordnet.zip', 'wordnet2021', 'wordnet2021.zip', 'wordnet31', 'wordnet31.zip', 'wordnet_ic', 'wordnet_ic.zip', 'words', 'words.zip', 'ycoe', 'ycoe.zip']\n"
     ]
    }
   ],
   "source": [
    "# list of all datasets names\n",
    "import os\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c32d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    }
   ],
   "source": [
    "# using \"names\" as an example\n",
    "from nltk.corpus import names\n",
    "\n",
    "# .words() returns a list of str\n",
    "print(names.words()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87868df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n"
     ]
    }
   ],
   "source": [
    "# using addtional inputs to get all male names\n",
    "print(names.words('male.txt')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "263d2470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
     ]
    }
   ],
   "source": [
    "# using addtional inputs to get all female names\n",
    "print(names.words('female.txt')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfc7e05a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gutenberg as other example\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# .fileids() returns a list of sub-file names\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebed99af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get specific text data by passing the file name to the words function\n",
    "gutenberg.words(\"shakespeare-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78980c85",
   "metadata": {},
   "source": [
    "## Tokenization through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "202824ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import word_tokenize function from nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6021a738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " ',',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " \"'understanding\",\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " '.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " '.',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'natural-language',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'and',\n",
       " 'natural-language',\n",
       " 'generation',\n",
       " '.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"\"\"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. \n",
    "\n",
    "The goal is a computer capable of 'understanding the contents of documents, including the contextual nuances of the language within them. \n",
    "\n",
    "The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves. \n",
    "\n",
    "Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(paragraph)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd17e592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 8, ',': 6, 'and': 6, 'language': 5, 'of': 5, '.': 4, 'natural': 3, 'in': 3, 'documents': 3, 'processing': 2, ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count frequency through freqdist\n",
    "from nltk.probability import FreqDist\n",
    "fd = FreqDist()\n",
    "for token in tokens:\n",
    "    fd[token.lower()] += 1\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eb5dd6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 8),\n",
       " (',', 6),\n",
       " ('and', 6),\n",
       " ('language', 5),\n",
       " ('of', 5),\n",
       " ('.', 4),\n",
       " ('natural', 3),\n",
       " ('in', 3),\n",
       " ('documents', 3),\n",
       " ('processing', 2)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using freqdist instead of a python dictionary gives us more functionalities;\n",
    "# such as finding the most common tokens through most_common() function\n",
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1d654550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blankline_tokenize\n",
    "from nltk.tokenize import blankline_tokenize\n",
    "lines = blankline_tokenize(paragraph)\n",
    "lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b65d0c4",
   "metadata": {},
   "source": [
    "## Grams in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4e3a9498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', '('),\n",
       " ('(', 'NLP'),\n",
       " ('NLP', ')')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "\n",
    "bigram = list(nltk.bigrams(tokens))\n",
    "bigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6bfe244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing'),\n",
       " ('language', 'processing', '('),\n",
       " ('processing', '(', 'NLP'),\n",
       " ('(', 'NLP', ')'),\n",
       " ('NLP', ')', 'is')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram = list(nltk.trigrams(tokens))\n",
    "trigram[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90430368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'language', 'processing', '(', 'NLP'),\n",
       " ('language', 'processing', '(', 'NLP', ')'),\n",
       " ('processing', '(', 'NLP', ')', 'is'),\n",
       " ('(', 'NLP', ')', 'is', 'a'),\n",
       " ('NLP', ')', 'is', 'a', 'subfield')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = list(nltk.ngrams(tokens, 5))\n",
    "ngram[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d17a4",
   "metadata": {},
   "source": [
    "## Stemming through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d6ec98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming refers to the process of finding the root/normalized form of a word\n",
    "\n",
    "# import porterstemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "ps.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "677a6523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hav'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import lancasterstemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "ls = LancasterStemmer()\n",
    "ls.stem(\"having\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c448afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import snowballstemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# for snowball stemmer an language must be provided in lowercase as the input of the constructor\n",
    "ss = SnowballStemmer(\"english\")\n",
    "ss.stem(\"having\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3049317",
   "metadata": {},
   "source": [
    "## Lemmatization through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e2ccc9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feel'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization refers to the process of finding the root/normalized form of a word, beyond steamming and with fewer roots\n",
    "\n",
    "# import lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize(\"feels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3c1dda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mice'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# difference between stemming and lemmatization\n",
    "ss.stem(\"mice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b4f379d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mouse'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize(\"mice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396891a",
   "metadata": {},
   "source": [
    "## Stopwords in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4158972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355b02b",
   "metadata": {},
   "source": [
    "## Part of speech through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ba633d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NNP'),\n",
       " (')', ')'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('subfield', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('linguistics', 'NNS'),\n",
       " (',', ','),\n",
       " ('computer', 'NN'),\n",
       " ('science', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('concerned', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('interactions', 'NNS'),\n",
       " ('between', 'IN'),\n",
       " ('computers', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('particular', 'JJ'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('program', 'NN'),\n",
       " ('computers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('process', 'VB'),\n",
       " ('and', 'CC'),\n",
       " ('analyze', 'VB'),\n",
       " ('large', 'JJ'),\n",
       " ('amounts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('goal', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('computer', 'NN'),\n",
       " ('capable', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"'understanding\", 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('contents', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('documents', 'NNS'),\n",
       " (',', ','),\n",
       " ('including', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('contextual', 'JJ'),\n",
       " ('nuances', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('language', 'NN'),\n",
       " ('within', 'IN'),\n",
       " ('them', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('technology', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('then', 'RB'),\n",
       " ('accurately', 'RB'),\n",
       " ('extract', 'JJ'),\n",
       " ('information', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('insights', 'NNS'),\n",
       " ('contained', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('documents', 'NNS'),\n",
       " ('as', 'RB'),\n",
       " ('well', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('categorize', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('organize', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('documents', 'NNS'),\n",
       " ('themselves', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('Challenges', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('frequently', 'RB'),\n",
       " ('involve', 'VBP'),\n",
       " ('speech', 'NN'),\n",
       " ('recognition', 'NN'),\n",
       " (',', ','),\n",
       " ('natural-language', 'JJ'),\n",
       " ('understanding', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('natural-language', 'JJ'),\n",
       " ('generation', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pos tag\n",
    "from nltk import pos_tag\n",
    "tags = pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all meanings of the tags, see https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f094a",
   "metadata": {},
   "source": [
    "## Named entity recognition through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22ba9922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  President/NNP\n",
      "  (PERSON Biden/NNP)\n",
      "  stepped/VBD\n",
      "  out/IN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (FACILITY White/NNP House/NNP)\n",
      "  and/CC\n",
      "  gave/VBD\n",
      "  a/DT\n",
      "  speech/NN\n",
      "  to/TO\n",
      "  the/DT\n",
      "  (ORGANIZATION US/NNP Navy/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# import ne chunk\n",
    "from nltk import ne_chunk\n",
    "\n",
    "t = \"President Biden stepped out of the White House and gave a speech to the US Navy.\"\n",
    "ne = ne_chunk(pos_tag(word_tokenize(t)))\n",
    "print(ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402bfe1",
   "metadata": {},
   "source": [
    "## Chunking through nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ab324c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    813\u001b[0m                     [\n\u001b[1;32m--> 814\u001b[1;33m                         find_binary(\n\u001b[0m\u001b[0;32m    815\u001b[0m                             \u001b[1;34m\"gs\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    686\u001b[0m ):\n\u001b[1;32m--> 687\u001b[1;33m     return next(\n\u001b[0m\u001b[0;32m    688\u001b[0m         find_binary_iter(\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    672\u001b[0m     \"\"\"\n\u001b[1;32m--> 673\u001b[1;33m     yield from find_file_iter(\n\u001b[0m\u001b[0;32m    674\u001b[0m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"=\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\n\\n{div}\\n{msg}\\n{div}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration parameters or set the PATH environment variable.\n===========================================================================",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m                 )\n\u001b[0;32m    832\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')]), ('.', '.')])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a regular expression corresponds to the specific chunk you wish to find\n",
    "re = r\"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# parse regular expression through regex parser\n",
    "from nltk import RegexpParser\n",
    "parsed = RegexpParser(re)\n",
    "\n",
    "t = \"The big cat ate the little mouse who was after fresh cheese.\"\n",
    "\n",
    "# returns a syntex tree\n",
    "chunks = parsed.parse(pos_tag(word_tokenize(t)))\n",
    "chunks\n",
    "\n",
    "# ignore the gs read error"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
